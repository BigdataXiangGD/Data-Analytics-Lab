---
title: "R Notebook"
output:
  pdf_document: default
  html_document:
    df_print: paged
---



### 1.Examination of the data


Load in the dataset and analysis, the aim of the assignment is to predict who is going to leave the organisation , our target column with which we can point our model to train on would be the "Attrition" column.


```{r}
#library(readxl)
#data <- read_excel("E:/GitHub/Data-Analytics/Employeeattrition.xlsx")
```
show the structure of the data and get the general information
```{r}
str(data)
```


As is seen in the Data






**Data quality checks**

Check if ther is any null values.
```{r}
sum(is.na(data))
```

required Libraries:
```{r}
library(ggplot2)
library(magrittr)
library(cluster)
library(dplyr)
library(ggplot2)
library(magrittr)
library(rpart)
library(maptree)
```

```{r}
data %>%
        group_by(Attrition) %>%
        tally() %>%
        ggplot(aes(x = Attrition, y = n,fill=Attrition)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        labs(x="Attrition", y="Count of Attriation")+
        ggtitle("Attrition")+
        geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))
```

//As we see here, 237/1470=0.16 % of the data label shows the "Yes" in Attrition. this problem should be handeled during the process because unbalanced dataset will bias the prediction model towards the more common class (here is 'NO'). There are different approaches for dealing with unbalanced data in machine learning like using more data (here is not possible), Resampling , changing the machine performance metric, using various algorithms etc.//
```{r}
data %>%
        group_by(JobRole, Attrition) %>%
        tally() %>%
        ggplot(aes(x = JobRole, y = n,fill=Attrition)) +
        geom_bar(stat = "identity") +
        theme_minimal()+
        labs(x="JobRole", y="Number Attriation")+
        ggtitle("Attrition according to JobRole")+
        geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))
```


### 2.Single tree model


Creating train & test sets
```{r}
set.seed(100)
sub<-sample(1:nrow(data),nrow(data)*0.7)
length(sub)
data_train<-data[sub,]
data_test<-data[-sub,]
dim(data_train)
dim(data_test)
```
Fit the single tree model
```{r}
fit <- rpart(Attrition ~ .,method="class",  
             data=data_train) 
```
Plot the tree model
```{r}
library(rpart.plot)
library(rattle)
fancyRpartPlot(fit)
```


Pruning a Decision Tree
```{r}
fit <- prune(tree = fit , cp = 0.01)
```
Predicton
```{r}
pred <- predict(fit,data_test,type="class") 
table(pred)
```
Result
```{r}
library(caret)
confusionMatrix(data_test$Attrition,pred)
```


### 3.Ensemble Technique 1 -- Random Forest
```{r}

```
We have to change the character into factor in order to fit the random forest model 
```{r}
cols <- c(2,3,5,8,12,16,18,22,23)
data[cols] <- lapply(data[cols], factor)
table(sapply(data,class))
```
```{r}
set.seed(100)
sub<-sample(1:nrow(data),nrow(data)*2/3)
length(sub)
train<-data[sub,]
test<-data[-sub,]
dim(train)
dim(test)
```
Fit the random forest model
```{r}
library(randomForest)
set.seed(2343)

# Random forest

fit.forest <- randomForest(Attrition ~., data = train)
rfpreds <- predict(fit.forest, test, type = "class")
varImpPlot(fit.forest)
```
```{r}
library(caret)
confusionMatrix(test$Attrition,rfpreds)
```

```{r}
plot(fit.forest)
```
```{r}
library(pROC)
rocrf <- roc(as.numeric(test$Attrition), as.numeric(rfpreds))
rocrf$auc
```
Random Forests are an improvement over bagged decision trees. 


A problem with decision trees like CART is that they are greedy. They choose which variable to split on using a greedy algorithm that minimizes error. As such, even with Bagging, the decision trees can have a lot of structural similarities and in turn have high correlation in their predictions.


Combining predictions from multiple models in ensembles works better if the predictions from the sub-models are uncorrelated or at best weakly correlated.

Random forest changes the algorithm for the way that the sub-trees are learned so that the resulting predictions from all of the subtrees have less correlation.


It is a simple tweak. In CART, when selecting a split point, the learning algorithm is allowed to look through all variables and all variable values in order to select the most optimal split-point. The random forest algorithm changes this procedure so that the learning algorithm is limited to a random sample of features of which to search.
```{r}

```





Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).
